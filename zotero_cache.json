[{"key": "QRMX3R4I", "version": 498, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/QRMX3R4I", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/QRMX3R4I", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/N3ZRTD3L", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 3161073}}, "meta": {"creatorSummary": "Wang et al.", "parsedDate": "2025-03-14", "numChildren": 3}, "data": {"key": "QRMX3R4I", "version": 498, "itemType": "preprint", "title": "VGGT: Visual Geometry Grounded Transformer", "creators": [{"creatorType": "author", "firstName": "Jianyuan", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Minghao", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Nikita", "lastName": "Karaev"}, {"creatorType": "author", "firstName": "Andrea", "lastName": "Vedaldi"}, {"creatorType": "author", "firstName": "Christian", "lastName": "Rupprecht"}, {"creatorType": "author", "firstName": "David", "lastName": "Novotny"}], "abstractNote": "We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2503.11651", "place": "", "date": "2025-03-14", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2503.11651", "citationKey": "", "url": "http://arxiv.org/abs/2503.11651", "accessDate": "2025-11-15T14:59:18Z", "archive": "", "archiveLocation": "", "shortTitle": "VGGT", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2503.11651 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-11-15T14:59:18Z", "dateModified": "2025-11-15T14:59:18Z"}}, {"key": "BCHBNT3H", "version": 477, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/BCHBNT3H", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/BCHBNT3H", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/TFPACQGD", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 8757857}}, "meta": {"creatorSummary": "Balestriero and LeCun", "parsedDate": "2025-11-13", "numChildren": 2}, "data": {"key": "BCHBNT3H", "version": 477, "itemType": "preprint", "title": "LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics", "creators": [{"creatorType": "author", "firstName": "Randall", "lastName": "Balestriero"}, {"creatorType": "author", "firstName": "Yann", "lastName": "LeCun"}], "abstractNote": "Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\\href{https://github.com/rbalestr-lab/lejepa}{GitHub repo}).", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2511.08544", "place": "", "date": "2025-11-13", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2511.08544", "citationKey": "", "url": "http://arxiv.org/abs/2511.08544", "accessDate": "2025-11-14T14:55:14Z", "archive": "", "archiveLocation": "", "shortTitle": "LeJEPA", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2511.08544 [cs]", "tags": [{"tag": "Computer Science - Artificial Intelligence", "type": 1}, {"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}, {"tag": "Computer Science - Machine Learning", "type": 1}, {"tag": "Statistics - Machine Learning", "type": 1}], "collections": ["RJF822R5"], "relations": {}, "dateAdded": "2025-11-14T14:55:15Z", "dateModified": "2025-11-14T14:55:15Z"}}, {"key": "QEC87TD5", "version": 482, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/QEC87TD5", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/QEC87TD5", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/WXVCBME2", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 2827414}}, "meta": {"creatorSummary": "Huang et al.", "parsedDate": "2025-11-14", "numChildren": 3}, "data": {"key": "QEC87TD5", "version": 482, "itemType": "preprint", "title": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation", "creators": [{"creatorType": "author", "firstName": "Xun", "lastName": "Huang"}, {"creatorType": "author", "firstName": "Shijia", "lastName": "Zhao"}, {"creatorType": "author", "firstName": "Yunxiang", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Xin", "lastName": "Lu"}, {"creatorType": "author", "firstName": "Wanfa", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Rongsheng", "lastName": "Qu"}, {"creatorType": "author", "firstName": "Weixin", "lastName": "Li"}, {"creatorType": "author", "firstName": "Yunhong", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Chenglu", "lastName": "Wen"}], "abstractNote": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2511.10376", "place": "", "date": "2025-11-14", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2511.10376", "citationKey": "", "url": "http://arxiv.org/abs/2511.10376", "accessDate": "2025-11-14T14:46:56Z", "archive": "", "archiveLocation": "", "shortTitle": "MSGNav", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2511.10376 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}, {"tag": "Computer Science - Robotics", "type": 1}], "collections": ["8U6JPDHT"], "relations": {}, "dateAdded": "2025-11-14T14:46:56Z", "dateModified": "2025-11-14T14:46:56Z"}}, {"key": "ADCD2736", "version": 488, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/ADCD2736", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/ADCD2736", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/QDJUF8DA", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 8449048}}, "meta": {"creatorSummary": "Zhu et al.", "parsedDate": "2025-07-30", "numChildren": 3}, "data": {"key": "ADCD2736", "version": 488, "itemType": "preprint", "title": "Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation", "creators": [{"creatorType": "author", "firstName": "Ziyu", "lastName": "Zhu"}, {"creatorType": "author", "firstName": "Xilin", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Yixuan", "lastName": "Li"}, {"creatorType": "author", "firstName": "Zhuofan", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Xiaojian", "lastName": "Ma"}, {"creatorType": "author", "firstName": "Yixin", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Baoxiong", "lastName": "Jia"}, {"creatorType": "author", "firstName": "Wei", "lastName": "Liang"}, {"creatorType": "author", "firstName": "Qian", "lastName": "Yu"}, {"creatorType": "author", "firstName": "Zhidong", "lastName": "Deng"}, {"creatorType": "author", "firstName": "Siyuan", "lastName": "Huang"}, {"creatorType": "author", "firstName": "Qing", "lastName": "Li"}], "abstractNote": "Embodied scene understanding requires not only comprehending visual-spatial information that has been observed but also determining where to explore next in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily focus on grounding objects in static observations from 3D reconstruction, such as meshes and point clouds, but lack the ability to actively perceive and explore their environment. To address this limitation, we introduce \\underline{\\textbf{M}}ove \\underline{\\textbf{t}}o \\underline{\\textbf{U}}nderstand (\\textbf{\\model}), a unified framework that integrates active perception with \\underline{\\textbf{3D}} vision-language learning, enabling embodied agents to effectively explore and understand their environment. This is achieved by three key innovations: 1) Online query-based representation learning, enabling direct spatial memory construction from RGB-D frames, eliminating the need for explicit 3D reconstruction. 2) A unified objective for grounding and exploring, which represents unexplored locations as frontier queries and jointly optimizes object grounding and frontier selection. 3) End-to-end trajectory learning that combines \\textbf{V}ision-\\textbf{L}anguage-\\textbf{E}xploration pre-training over a million diverse trajectories collected from both simulated and real-world RGB-D sequences. Extensive evaluations across various embodied navigation and question-answering benchmarks show that MTU3D outperforms state-of-the-art reinforcement learning and modular navigation approaches by 14\\%, 23\\%, 9\\%, and 2\\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA, respectively. \\model's versatility enables navigation using diverse input modalities, including categories, language descriptions, and reference images. These findings highlight the importance of bridging visual grounding and exploration for embodied intelligence.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2507.04047", "place": "", "date": "2025-07-30", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2507.04047", "citationKey": "", "url": "http://arxiv.org/abs/2507.04047", "accessDate": "2025-11-11T13:01:12Z", "archive": "", "archiveLocation": "", "shortTitle": "Move to Understand a 3D Scene", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2507.04047 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": ["8U6JPDHT"], "relations": {}, "dateAdded": "2025-11-11T13:01:12Z", "dateModified": "2025-11-11T13:01:12Z"}}, {"key": "REY7B9SI", "version": 321, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/REY7B9SI", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/REY7B9SI", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/CMKNBFHA", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 20345933}}, "meta": {"creatorSummary": "Mirlach et al.", "parsedDate": "2025-07-30", "numChildren": 3}, "data": {"key": "REY7B9SI", "version": 321, "itemType": "preprint", "title": "R-LiViT: A LiDAR-Visual-Thermal Dataset Enabling Vulnerable Road User Focused Roadside Perception", "creators": [{"creatorType": "author", "firstName": "Jonas", "lastName": "Mirlach"}, {"creatorType": "author", "firstName": "Lei", "lastName": "Wan"}, {"creatorType": "author", "firstName": "Andreas", "lastName": "Wiedholz"}, {"creatorType": "author", "firstName": "Hannan Ejaz", "lastName": "Keen"}, {"creatorType": "author", "firstName": "Andreas", "lastName": "Eich"}], "abstractNote": "In autonomous driving, the integration of roadside perception systems is essential for overcoming occlusion challenges and enhancing the safety of Vulnerable Road Users(VRUs). While LiDAR and visual (RGB) sensors are commonly used, thermal imaging remains underrepresented in datasets, despite its acknowledged advantages for VRU detection in extreme lighting conditions. In this paper, we present R-LiViT, the first dataset to combine LiDAR, RGB, and thermal imaging from a roadside perspective, with a strong focus on VRUs. R-LiViT captures three intersections during both day and night, ensuring a diverse dataset. It includes 10,000 LiDAR frames and 2,400 temporally and spatially aligned RGB and thermal images across 150 traffic scenarios, with 7 and 8 annotated classes respectively, providing a comprehensive resource for tasks such as object detection and tracking. The dataset and the code for reproducing our evaluation results are made publicly available.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2503.17122", "place": "", "date": "2025-07-30", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2503.17122", "citationKey": "", "url": "http://arxiv.org/abs/2503.17122", "accessDate": "2025-11-11T13:00:45Z", "archive": "", "archiveLocation": "", "shortTitle": "R-LiViT", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2503.17122 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-11-11T13:00:45Z", "dateModified": "2025-11-11T13:00:45Z"}}, {"key": "W2E2J66N", "version": 317, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/W2E2J66N", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/W2E2J66N", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/3LSIX8EX", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 10084110}}, "meta": {"creatorSummary": "Lu et al.", "parsedDate": "2025-07-16", "numChildren": 3}, "data": {"key": "W2E2J66N", "version": 317, "itemType": "preprint", "title": "ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving", "creators": [{"creatorType": "author", "firstName": "Yuhang", "lastName": "Lu"}, {"creatorType": "author", "firstName": "Jiadong", "lastName": "Tu"}, {"creatorType": "author", "firstName": "Yuexin", "lastName": "Ma"}, {"creatorType": "author", "firstName": "Xinge", "lastName": "Zhu"}], "abstractNote": "End-to-end autonomous driving has emerged as a promising approach to unify perception, prediction, and planning within a single framework, reducing information loss and improving adaptability. However, existing methods often rely on fixed and sparse trajectory supervision, limiting their ability to capture the hierarchical reasoning process that human drivers naturally employ. To bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning framework that structures decision-making in autonomous driving based on the three-tier human cognitive model: Driving Strategy, Driving Decision, and Driving Operation, where Vision-Language Models (VLMs) are incorporated to enhance situational awareness and structured reasoning across these levels. Specifically, we introduce: (1) the Strategic Reasoning Injector, which formulates high-level driving strategies by interpreting complex traffic contexts from VLM-generated insights; (2) the Tactical Reasoning Integrator, which refines strategic intent into interpretable tactical choices such as lane changes, overtaking, and speed adjustments; and (3) the Hierarchical Trajectory Decoder, which progressively translates tactical decisions into precise control actions for smooth and human-like trajectory execution. Extensive evaluations show that integrating our framework improves planning accuracy and safety by over 30%, making end-to-end autonomous driving more interpretable and aligned with human-like hierarchical reasoning. The project page can be found at: \\href{https://4dvlab.github.io/project_page/realad}{\\texttt{4dvlab.github.io/project\\_page/realad}}", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2507.12499", "place": "", "date": "2025-07-16", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2507.12499", "citationKey": "", "url": "http://arxiv.org/abs/2507.12499", "accessDate": "2025-11-11T12:56:27Z", "archive": "", "archiveLocation": "", "shortTitle": "ReAL-AD", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2507.12499 [cs]", "tags": [{"tag": "Computer Science - Machine Learning", "type": 1}, {"tag": "Computer Science - Robotics", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-11-11T12:56:27Z", "dateModified": "2025-11-11T12:56:27Z"}}, {"key": "TBK8ZMIL", "version": 311, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/TBK8ZMIL", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/TBK8ZMIL", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/3JEWPHKM", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 11226635}}, "meta": {"creatorSummary": "Jiang et al.", "parsedDate": "2025-07-30", "numChildren": 3}, "data": {"key": "TBK8ZMIL", "version": 311, "itemType": "preprint", "title": "Learning to See in the Extremely Dark", "creators": [{"creatorType": "author", "firstName": "Hai", "lastName": "Jiang"}, {"creatorType": "author", "firstName": "Binhao", "lastName": "Guan"}, {"creatorType": "author", "firstName": "Zhen", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Xiaohong", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Jian", "lastName": "Yu"}, {"creatorType": "author", "firstName": "Zheng", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Songchen", "lastName": "Han"}, {"creatorType": "author", "firstName": "Shuaicheng", "lastName": "Liu"}], "abstractNote": "Learning-based methods have made promising advances in low-light RAW image enhancement, while their capability to extremely dark scenes where the environmental illuminance drops as low as 0.0001 lux remains to be explored due to the lack of corresponding datasets. To this end, we propose a paired-to-paired data synthesis pipeline capable of generating well-calibrated extremely low-light RAW images at three precise illuminance ranges of 0.01-0.1 lux, 0.001-0.01 lux, and 0.0001-0.001 lux, together with high-quality sRGB references to comprise a large-scale paired dataset named See-in-the-Extremely-Dark (SIED) to benchmark low-light RAW image enhancement approaches. Furthermore, we propose a diffusion-based framework that leverages the generative ability and intrinsic denoising property of diffusion models to restore visually pleasing results from extremely low-SNR RAW inputs, in which an Adaptive Illumination Correction Module (AICM) and a color consistency loss are introduced to ensure accurate exposure correction and color restoration. Extensive experiments on the proposed SIED and publicly available benchmarks demonstrate the effectiveness of our method. The code and dataset are available at https://github.com/JianghaiSCU/SIED.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2506.21132", "place": "", "date": "2025-07-30", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2506.21132", "citationKey": "", "url": "http://arxiv.org/abs/2506.21132", "accessDate": "2025-11-11T12:54:29Z", "archive": "", "archiveLocation": "", "shortTitle": "", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2506.21132 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-11-11T12:54:29Z", "dateModified": "2025-11-11T12:54:29Z"}}, {"key": "2CH5GEMP", "version": 482, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/2CH5GEMP", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/2CH5GEMP", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/Y9Y6J9TR", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 411296}}, "meta": {"creatorSummary": "Gao et al.", "parsedDate": "2025-11-04", "numChildren": 2}, "data": {"key": "2CH5GEMP", "version": 482, "itemType": "preprint", "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment", "creators": [{"creatorType": "author", "firstName": "Yu", "lastName": "Gao"}, {"creatorType": "author", "firstName": "Anqing", "lastName": "Jiang"}, {"creatorType": "author", "firstName": "Yiru", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Wang", "lastName": "Jijun"}, {"creatorType": "author", "firstName": "Hao", "lastName": "Jiang"}, {"creatorType": "author", "firstName": "Zhigang", "lastName": "Sun"}, {"creatorType": "author", "firstName": "Heng", "lastName": "Yuwen"}, {"creatorType": "author", "firstName": "Wang", "lastName": "Shuo"}, {"creatorType": "author", "firstName": "Hao", "lastName": "Zhao"}, {"creatorType": "author", "firstName": "Sun", "lastName": "Hao"}], "abstractNote": "Conventional end-to-end (E2E) driving models are effective at generating physically plausible trajectories, but often fail to generalize to long-tail scenarios due to the lack of essential world knowledge to understand and reason about surrounding environments. In contrast, Vision-Language-Action (VLA) models leverage world knowledge to handle challenging cases, but their limited 3D reasoning capability can lead to physically infeasible actions. In this work we introduce DiffVLA++, an enhanced autonomous driving framework that explicitly bridges cognitive reasoning and E2E planning through metric-guided alignment. First, we build a VLA module directly generating semantically grounded driving trajectories. Second, we design an E2E module with a dense trajectory vocabulary that ensures physical feasibility. Third, and most critically, we introduce a metric-guided trajectory scorer that guides and aligns the outputs of the VLA and E2E modules, thereby integrating their complementary strengths. The experiment on the ICCV 2025 Autonomous Grand Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2510.17148", "place": "", "date": "2025-11-04", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2510.17148", "citationKey": "", "url": "http://arxiv.org/abs/2510.17148", "accessDate": "2025-11-11T11:26:58Z", "archive": "", "archiveLocation": "", "shortTitle": "DiffVLA++", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2510.17148 [cs]\nversion: 4", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}, {"tag": "Computer Science - Robotics", "type": 1}], "collections": ["4KAX5SGB"], "relations": {}, "dateAdded": "2025-11-11T11:26:58Z", "dateModified": "2025-11-11T11:26:58Z"}}, {"key": "DMLMY9ZE", "version": 493, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/DMLMY9ZE", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/DMLMY9ZE", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/8NRZAT7F", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 764745}}, "meta": {"creatorSummary": "Yao et al.", "parsedDate": "2025-05-22", "numChildren": 3}, "data": {"key": "DMLMY9ZE", "version": 493, "itemType": "preprint", "title": "LiloDriver: A Lifelong Learning Framework for Closed-loop Motion Planning in Long-tail Autonomous Driving Scenarios", "creators": [{"creatorType": "author", "firstName": "Huaiyuan", "lastName": "Yao"}, {"creatorType": "author", "firstName": "Pengfei", "lastName": "Li"}, {"creatorType": "author", "firstName": "Bu", "lastName": "Jin"}, {"creatorType": "author", "firstName": "Yupeng", "lastName": "Zheng"}, {"creatorType": "author", "firstName": "An", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Lisen", "lastName": "Mu"}, {"creatorType": "author", "firstName": "Qing", "lastName": "Su"}, {"creatorType": "author", "firstName": "Qian", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Yilun", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Peng", "lastName": "Li"}], "abstractNote": "Recent advances in autonomous driving research towards motion planners that are robust, safe, and adaptive. However, existing rule-based and data-driven planners lack adaptability to long-tail scenarios, while knowledge-driven methods offer strong reasoning but face challenges in representation, control, and real-world evaluation. To address these challenges, we present LiloDriver, a lifelong learning framework for closed-loop motion planning in long-tail autonomous driving scenarios. By integrating large language models (LLMs) with a memory-augmented planner generation system, LiloDriver continuously adapts to new scenarios without retraining. It features a four-stage architecture including perception, scene encoding, memory-based strategy refinement, and LLM-guided reasoning. Evaluated on the nuPlan benchmark, LiloDriver achieves superior performance in both common and rare driving scenarios, outperforming static rule-based and learning-based planners. Our results highlight the effectiveness of combining structured memory and LLM reasoning to enable scalable, human-like motion planning in real-world autonomous driving. Our code is available at https://github.com/Hyan-Yao/LiloDriver.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2505.17209", "place": "", "date": "2025-05-22", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2505.17209", "citationKey": "", "url": "http://arxiv.org/abs/2505.17209", "accessDate": "2025-11-11T11:26:34Z", "archive": "", "archiveLocation": "", "shortTitle": "LiloDriver", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2505.17209 [cs]", "tags": [{"tag": "Computer Science - Artificial Intelligence", "type": 1}, {"tag": "Computer Science - Robotics", "type": 1}], "collections": ["4KAX5SGB"], "relations": {}, "dateAdded": "2025-11-11T11:26:34Z", "dateModified": "2025-11-11T11:26:34Z"}}, {"key": "MRTHKTKV", "version": 293, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/MRTHKTKV", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/MRTHKTKV", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/9FPEERKW", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 833791}}, "meta": {"creatorSummary": "Chen et al.", "parsedDate": "2025-02", "numChildren": 1}, "data": {"key": "MRTHKTKV", "version": 293, "itemType": "conferencePaper", "title": "Automated Evaluation of Large Vision-Language Models on Self-Driving Corner Cases", "creators": [{"creatorType": "author", "firstName": "Kai", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Yanze", "lastName": "Li"}, {"creatorType": "author", "firstName": "Wenhua", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Yanxin", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Pengxiang", "lastName": "Li"}, {"creatorType": "author", "firstName": "Ruiyuan", "lastName": "Gao"}, {"creatorType": "author", "firstName": "Lanqing", "lastName": "Hong"}, {"creatorType": "author", "firstName": "Meng", "lastName": "Tian"}, {"creatorType": "author", "firstName": "Xinhai", "lastName": "Zhao"}, {"creatorType": "author", "firstName": "Zhenguo", "lastName": "Li"}, {"creatorType": "author", "firstName": "Dit-Yan", "lastName": "Yeung"}, {"creatorType": "author", "firstName": "Huchuan", "lastName": "Lu"}, {"creatorType": "author", "firstName": "Xu", "lastName": "Jia"}], "abstractNote": "Large Vision-Language Models (LVLMs) have received widespread attentions for advancing the interpretable self-driving. Existing evaluations of LVLMs primarily focus on multi-faceted capabilities in natural circumstances, lacking automated and quantifiable assessment for self-driving, let alone the severe road corner cases. In this work, we propose CODA-LM, the very first benchmark for the automatic evaluation of LVLMs for self-driving corner cases. We adopt a hierarchical data structure and prompt powerful LVLMs to analyze complex driving scenes and generate high-quality pre-annotations for the human annotators, while for LVLM evaluation, we show that using the text-only large language models (LLMs) as judges reveals even better alignment with human preferences than the LVLM judges. Moreover, with our CODA-LM, we build CODA-VLM, a new driving LVLM surpassing all open-sourced counterparts on CODA-LM. Our CODA-VLM performs comparably with GPT-4V, even surpassing GPT-4V by +21.42% on the regional perception task. We hope CODA-LM can become the catalyst to promote interpretable self-driving empowered by LVLMs.", "date": "2025-02", "proceedingsTitle": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "conferenceName": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "place": "", "publisher": "", "volume": "", "pages": "7817-7826", "series": "", "language": "", "DOI": "10.1109/WACV61041.2025.00759", "ISBN": "", "shortTitle": "", "url": "https://ieeexplore.ieee.org/abstract/document/10943406", "accessDate": "2025-11-11T02:37:24Z", "archive": "", "archiveLocation": "", "libraryCatalog": "IEEE Xplore", "callNumber": "", "rights": "", "extra": "ISSN: 2642-9381", "tags": [{"tag": "Autonomous vehicles", "type": 1}, {"tag": "Benchmark testing", "type": 1}, {"tag": "Catalysts", "type": 1}, {"tag": "Computer vision", "type": 1}, {"tag": "Data structures", "type": 1}, {"tag": "Large language models", "type": 1}, {"tag": "Reliability", "type": 1}, {"tag": "Roads", "type": 1}, {"tag": "autonomous driving", "type": 1}, {"tag": "corner cases", "type": 1}, {"tag": "interpretable autonomous agents", "type": 1}, {"tag": "large vision language models", "type": 1}, {"tag": "multimodality", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-11-11T02:37:24Z", "dateModified": "2025-11-11T02:37:24Z"}}, {"key": "IFEJJ8K6", "version": 495, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/IFEJJ8K6", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/IFEJJ8K6", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/5DQHKACB", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 3161855}}, "meta": {"creatorSummary": "Wang et al.", "parsedDate": "2025-10-19", "numChildren": 3}, "data": {"key": "IFEJJ8K6", "version": 495, "itemType": "preprint", "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "creators": [{"creatorType": "author", "firstName": "Kangrui", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Pingyue", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Zihan", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Yaning", "lastName": "Gao"}, {"creatorType": "author", "firstName": "Linjie", "lastName": "Li"}, {"creatorType": "author", "firstName": "Qineng", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Hanyang", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Chi", "lastName": "Wan"}, {"creatorType": "author", "firstName": "Yiping", "lastName": "Lu"}, {"creatorType": "author", "firstName": "Zhengyuan", "lastName": "Yang"}, {"creatorType": "author", "firstName": "Lijuan", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Ranjay", "lastName": "Krishna"}, {"creatorType": "author", "firstName": "Jiajun", "lastName": "Wu"}, {"creatorType": "author", "firstName": "Li", "lastName": "Fei-Fei"}, {"creatorType": "author", "firstName": "Yejin", "lastName": "Choi"}, {"creatorType": "author", "firstName": "Manling", "lastName": "Li"}], "abstractNote": "A key challenge in training Vision-Language Model (VLM) agents, compared to Language Model (LLM) agents, lies in the shift from textual states to complex visual observations. This transition introduces partial observability and demands robust world modeling. We ask: Can VLM agents construct internal world models through explicit visual state reasoning? To address this question, we architecturally enforce and reward the agent's reasoning process via reinforcement learning (RL), formulating it as a Partially Observable Markov Decision Process (POMDP). We find that decomposing the agent's reasoning into State Estimation (\"what is the current state?\") and Transition Modeling (\"what comes next?\") is critical for success, as demonstrated through five reasoning strategies. Our investigation into how agents represent internal beliefs reveals that the optimal representation is task-dependent: Natural Language excels at capturing semantic relationships in general tasks, while Structured formats are indispensable for precise manipulation and control. Building on these insights, we design a World Modeling Reward that provides dense, turn-level supervision for accurate state prediction, and introduce Bi-Level General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment. Through this form of visual state reasoning, a 3B-parameter model achieves a score of 0.82 across five diverse agent benchmarks, representing a 3$\\times$ improvement over its untrained counterpart (0.21) and outperforming proprietary reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5 (0.62). All experiments are conducted within our VAGEN framework, a scalable system for training and analyzing multi-turn VLM agents in diverse visual environments. Code and data are publicly available at https://vagen-ai.github.io.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2510.16907", "place": "", "date": "2025-10-19", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2510.16907", "citationKey": "", "url": "http://arxiv.org/abs/2510.16907", "accessDate": "2025-11-04T15:56:16Z", "archive": "", "archiveLocation": "", "shortTitle": "VAGEN", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2510.16907 [cs]", "tags": [{"tag": "Computer Science - Artificial Intelligence", "type": 1}, {"tag": "Computer Science - Computation and Language", "type": 1}], "collections": ["8U6JPDHT", "R5TA74HW"], "relations": {}, "dateAdded": "2025-11-04T15:56:16Z", "dateModified": "2025-11-04T15:56:16Z"}}, {"key": "83DM5LVD", "version": 490, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/83DM5LVD", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/83DM5LVD", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/RVKLMDR2", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 21695820}}, "meta": {"creatorSummary": "Zhang et al.", "parsedDate": "2025-06-30", "numChildren": 3}, "data": {"key": "83DM5LVD", "version": 490, "itemType": "preprint", "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "creators": [{"creatorType": "author", "firstName": "Kaiwen", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Zhenyu", "lastName": "Tang"}, {"creatorType": "author", "firstName": "Xiaotao", "lastName": "Hu"}, {"creatorType": "author", "firstName": "Xingang", "lastName": "Pan"}, {"creatorType": "author", "firstName": "Xiaoyang", "lastName": "Guo"}, {"creatorType": "author", "firstName": "Yuan", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Jingwei", "lastName": "Huang"}, {"creatorType": "author", "firstName": "Li", "lastName": "Yuan"}, {"creatorType": "author", "firstName": "Qian", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Xiao-Xiao", "lastName": "Long"}, {"creatorType": "author", "firstName": "Xun", "lastName": "Cao"}, {"creatorType": "author", "firstName": "Wei", "lastName": "Yin"}], "abstractNote": "Diffusion models have demonstrated exceptional visual quality in video generation, making them promising for autonomous driving world modeling. However, existing video diffusion-based world models struggle with flexible-length, long-horizon predictions and integrating trajectory planning. This is because conventional video diffusion models rely on global joint distribution modeling of fixed-length frame sequences rather than sequentially constructing localized distributions at each timestep. In this work, we propose Epona, an autoregressive diffusion world model that enables localized spatiotemporal distribution modeling through two key innovations: 1) Decoupled spatiotemporal factorization that separates temporal dynamics modeling from fine-grained future world generation, and 2) Modular trajectory and video prediction that seamlessly integrate motion planning with visual modeling in an end-to-end framework. Our architecture enables high-resolution, long-duration generation while introducing a novel chain-of-forward training strategy to address error accumulation in autoregressive loops. Experimental results demonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes longer prediction duration compared to prior works. The learned world model further serves as a real-time motion planner, outperforming strong end-to-end planners on NAVSIM benchmarks. Code will be publicly available at \\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2506.24113", "place": "", "date": "2025-06-30", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2506.24113", "citationKey": "", "url": "http://arxiv.org/abs/2506.24113", "accessDate": "2025-11-04T15:55:41Z", "archive": "", "archiveLocation": "", "shortTitle": "Epona", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2506.24113 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": ["4KAX5SGB"], "relations": {}, "dateAdded": "2025-11-04T15:55:41Z", "dateModified": "2025-11-04T15:55:41Z"}}, {"key": "F5UQLZ3A", "version": 496, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/F5UQLZ3A", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/F5UQLZ3A", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/LIVXJGH5", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 3196047}}, "meta": {"creatorSummary": "Li et al.", "parsedDate": "2025-10-14", "numChildren": 2}, "data": {"key": "F5UQLZ3A", "version": 496, "itemType": "preprint", "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving", "creators": [{"creatorType": "author", "firstName": "Yingyan", "lastName": "Li"}, {"creatorType": "author", "firstName": "Shuyao", "lastName": "Shang"}, {"creatorType": "author", "firstName": "Weisong", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Bing", "lastName": "Zhan"}, {"creatorType": "author", "firstName": "Haochen", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Yuqi", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Yuntao", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Xiaoman", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Yasong", "lastName": "An"}, {"creatorType": "author", "firstName": "Chufeng", "lastName": "Tang"}, {"creatorType": "author", "firstName": "Lu", "lastName": "Hou"}, {"creatorType": "author", "firstName": "Lue", "lastName": "Fan"}, {"creatorType": "author", "firstName": "Zhaoxiang", "lastName": "Zhang"}], "abstractNote": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2510.12796", "place": "", "date": "2025-10-14", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2510.12796", "citationKey": "", "url": "http://arxiv.org/abs/2510.12796", "accessDate": "2025-11-04T15:54:35Z", "archive": "", "archiveLocation": "", "shortTitle": "DriveVLA-W0", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2510.12796 [cs]", "tags": [{"tag": "Computer Science - Artificial Intelligence", "type": 1}, {"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": ["4KAX5SGB"], "relations": {}, "dateAdded": "2025-11-04T15:54:35Z", "dateModified": "2025-11-04T15:54:35Z"}}, {"key": "6ZRQWGGR", "version": 237, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/6ZRQWGGR", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/6ZRQWGGR", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/ESJRX5JU", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 16403999}}, "meta": {"creatorSummary": "Pouransari et al.", "parsedDate": "2025-10-06", "numChildren": 2}, "data": {"key": "6ZRQWGGR", "version": 237, "itemType": "preprint", "title": "Pretraining with hierarchical memories: separating long-tail and common knowledge", "creators": [{"creatorType": "author", "firstName": "Hadi", "lastName": "Pouransari"}, {"creatorType": "author", "firstName": "David", "lastName": "Grangier"}, {"creatorType": "author", "firstName": "C.", "lastName": "Thomas"}, {"creatorType": "author", "firstName": "Michael", "lastName": "Kirchhof"}, {"creatorType": "author", "firstName": "Oncel", "lastName": "Tuzel"}], "abstractNote": "The impressive performance gains of modern language models currently rely on scaling parameters: larger models store more world knowledge and reason better. Yet compressing all world knowledge into parameters is unnecessary, as only a fraction is used per prompt, and impractical for edge devices with limited inference-time memory and compute. We address this shortcoming by a memory-augmented architecture and a pretraining strategy aligned with existing hardware paradigms. We introduce small language models that access large hierarchical parametric memory banks encoding world knowledge. During pretraining and inference, we fetch a small, context-dependent memory block and add it to the model. Our pretraining learns to store long-tail world knowledge in the memory parameters, while the small language model acts as an anchor capturing common knowledge and general reasoning abilities. Through trillion-token-scale experiments, we show significant gains: a 160M-parameters model augmented with an 18M-parameters memory fetched from a 4.6B memory bank obtains comparable performance to a regular model with more than 2x the parameters. Through extensive experiments, we study the optimal type and size of parametric memories in transformers, scaling them to over 21B parameters. We find that our proposed hierarchical feed-forward memories work robustly across transformer architectures, whether added during pretraining or post-hoc.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2510.02375", "place": "", "date": "2025-10-06", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2510.02375", "citationKey": "", "url": "http://arxiv.org/abs/2510.02375", "accessDate": "2025-11-04T14:31:58Z", "archive": "", "archiveLocation": "", "shortTitle": "Pretraining with hierarchical memories", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2510.02375 [cs]", "tags": [{"tag": "Computer Science - Artificial Intelligence", "type": 1}, {"tag": "Computer Science - Computation and Language", "type": 1}, {"tag": "Computer Science - Machine Learning", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-11-04T14:31:58Z", "dateModified": "2025-11-04T14:31:58Z"}}, {"key": "GU34KM5L", "version": 497, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/GU34KM5L", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/GU34KM5L", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/JEGJQSB4", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 12713290}}, "meta": {"creatorSummary": "Li et al.", "parsedDate": "2025-03-04", "numChildren": 3}, "data": {"key": "GU34KM5L", "version": 497, "itemType": "preprint", "title": "Open-World Reinforcement Learning over Long Short-Term Imagination", "creators": [{"creatorType": "author", "firstName": "Jiajian", "lastName": "Li"}, {"creatorType": "author", "firstName": "Qi", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Yunbo", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Xin", "lastName": "Jin"}, {"creatorType": "author", "firstName": "Yang", "lastName": "Li"}, {"creatorType": "author", "firstName": "Wenjun", "lastName": "Zeng"}, {"creatorType": "author", "firstName": "Xiaokang", "lastName": "Yang"}], "abstractNote": "Training visual reinforcement learning agents in a high-dimensional open world presents significant challenges. While various model-based methods have improved sample efficiency by learning interactive world models, these agents tend to be \"short-sighted\", as they are typically trained on short snippets of imagined experiences. We argue that the primary challenge in open-world decision-making is improving the exploration efficiency across a vast state space, especially for tasks that demand consideration of long-horizon payoffs. In this paper, we present LS-Imagine, which extends the imagination horizon within a limited number of state transition steps, enabling the agent to explore behaviors that potentially lead to promising long-term feedback. The foundation of our approach is to build a $\\textit{long short-term world model}$. To achieve this, we simulate goal-conditioned jumpy state transitions and compute corresponding affordance maps by zooming in on specific areas within single images. This facilitates the integration of direct long-term values into behavior learning. Our method demonstrates significant improvements over state-of-the-art techniques in MineDojo.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2410.03618", "place": "", "date": "2025-03-04", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2410.03618", "citationKey": "", "url": "http://arxiv.org/abs/2410.03618", "accessDate": "2025-10-26T06:25:51Z", "archive": "", "archiveLocation": "", "shortTitle": "", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2410.03618 [cs]", "tags": [{"tag": "Computer Science - Machine Learning", "type": 1}], "collections": ["R5TA74HW"], "relations": {}, "dateAdded": "2025-10-26T06:25:51Z", "dateModified": "2025-10-26T06:25:51Z"}}, {"key": "JLFAYWMA", "version": 109, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/JLFAYWMA", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/JLFAYWMA", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/I2QUIWM2", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 9267939}}, "meta": {"creatorSummary": "Lei et al.", "parsedDate": "2024-11-29", "numChildren": 3}, "data": {"key": "JLFAYWMA", "version": 109, "itemType": "preprint", "title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds", "creators": [{"creatorType": "author", "firstName": "Jiahui", "lastName": "Lei"}, {"creatorType": "author", "firstName": "Yijia", "lastName": "Weng"}, {"creatorType": "author", "firstName": "Adam", "lastName": "Harley"}, {"creatorType": "author", "firstName": "Leonidas", "lastName": "Guibas"}, {"creatorType": "author", "firstName": "Kostas", "lastName": "Daniilidis"}], "abstractNote": "We introduce 4D Motion Scaffolds (MoSca), a modern 4D reconstruction system designed to reconstruct and synthesize novel views of dynamic scenes from monocular videos captured casually in the wild. To address such a challenging and ill-posed inverse problem, we leverage prior knowledge from foundational vision models and lift the video data to a novel Motion Scaffold (MoSca) representation, which compactly and smoothly encodes the underlying motions/deformations. The scene geometry and appearance are then disentangled from the deformation field and are encoded by globally fusing the Gaussians anchored onto the MoSca and optimized via Gaussian Splatting. Additionally, camera focal length and poses can be solved using bundle adjustment without the need of any other pose estimation tools. Experiments demonstrate state-of-the-art performance on dynamic rendering benchmarks and its effectiveness on real videos.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2405.17421", "place": "", "date": "2024-11-29", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2405.17421", "citationKey": "", "url": "http://arxiv.org/abs/2405.17421", "accessDate": "2025-10-26T06:01:03Z", "archive": "", "archiveLocation": "", "shortTitle": "MoSca", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2405.17421 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}, {"tag": "Computer Science - Graphics", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-10-26T06:01:03Z", "dateModified": "2025-10-26T06:01:03Z"}}, {"key": "HIMHN3QN", "version": 492, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/HIMHN3QN", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/HIMHN3QN", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/NWU36ABQ", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 5052985}}, "meta": {"creatorSummary": "Yuan et al.", "parsedDate": "2024-09-23", "numChildren": 3}, "data": {"key": "HIMHN3QN", "version": 492, "itemType": "preprint", "title": "General Flow as Foundation Affordance for Scalable Robot Learning", "creators": [{"creatorType": "author", "firstName": "Chengbo", "lastName": "Yuan"}, {"creatorType": "author", "firstName": "Chuan", "lastName": "Wen"}, {"creatorType": "author", "firstName": "Tong", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Yang", "lastName": "Gao"}], "abstractNote": "We address the challenge of acquiring real-world manipulation skills with a scalable framework. We hold the belief that identifying an appropriate prediction target capable of leveraging large-scale datasets is crucial for achieving efficient and universal learning. Therefore, we propose to utilize 3D flow, which represents the future trajectories of 3D points on objects of interest, as an ideal prediction target. To exploit scalable data resources, we turn our attention to human videos. We develop, for the first time, a language-conditioned 3D flow prediction model directly from large-scale RGBD human video datasets. Our predicted flow offers actionable guidance, thus facilitating zero-shot skill transfer in real-world scenarios. We deploy our method with a policy based on closed-loop flow prediction. Remarkably, without any in-domain finetuning, our method achieves an impressive 81\\% success rate in zero-shot human-to-robot skill transfer, covering 18 tasks in 6 scenes. Our framework features the following benefits: (1) scalability: leveraging cross-embodiment data resources; (2) wide application: multiple object categories, including rigid, articulated, and soft bodies; (3) stable skill transfer: providing actionable guidance with a small inference domain-gap. Code, data, and supplementary materials are available https://general-flow.github.io", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2401.11439", "place": "", "date": "2024-09-23", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2401.11439", "citationKey": "", "url": "http://arxiv.org/abs/2401.11439", "accessDate": "2025-10-26T05:48:14Z", "archive": "", "archiveLocation": "", "shortTitle": "", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2401.11439 [cs]", "tags": [{"tag": "Computer Science - Artificial Intelligence", "type": 1}, {"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}, {"tag": "Computer Science - Robotics", "type": 1}], "collections": ["8U6JPDHT"], "relations": {}, "dateAdded": "2025-10-26T05:48:15Z", "dateModified": "2025-10-26T05:48:15Z"}}, {"key": "LQBAWCX3", "version": 86, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/LQBAWCX3", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/LQBAWCX3", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/FAMI43F7", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 23877395}}, "meta": {"creatorSummary": "Ke et al.", "parsedDate": "2024-04-03", "numChildren": 3}, "data": {"key": "LQBAWCX3", "version": 86, "itemType": "preprint", "title": "Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation", "creators": [{"creatorType": "author", "firstName": "Bingxin", "lastName": "Ke"}, {"creatorType": "author", "firstName": "Anton", "lastName": "Obukhov"}, {"creatorType": "author", "firstName": "Shengyu", "lastName": "Huang"}, {"creatorType": "author", "firstName": "Nando", "lastName": "Metzger"}, {"creatorType": "author", "firstName": "Rodrigo Caye", "lastName": "Daudt"}, {"creatorType": "author", "firstName": "Konrad", "lastName": "Schindler"}], "abstractNote": "Monocular depth estimation is a fundamental computer vision task. Recovering 3D depth from a single image is geometrically ill-posed and requires scene understanding, so it is not surprising that the rise of deep learning has led to a breakthrough. The impressive progress of monocular depth estimators has mirrored the growth in model capacity, from relatively modest CNNs to large Transformer architectures. Still, monocular depth estimators tend to struggle when presented with images with unfamiliar content and layout, since their knowledge of the visual world is restricted by the data seen during training, and challenged by zero-shot generalization to new domains. This motivates us to explore whether the extensive priors captured in recent generative diffusion models can enable better, more generalizable depth estimation. We introduce Marigold, a method for affine-invariant monocular depth estimation that is derived from Stable Diffusion and retains its rich prior knowledge. The estimator can be fine-tuned in a couple of days on a single GPU using only synthetic training data. It delivers state-of-the-art performance across a wide range of datasets, including over 20% performance gains in specific cases. Project page: https://marigoldmonodepth.github.io.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2312.02145", "place": "", "date": "2024-04-03", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2312.02145", "citationKey": "", "url": "http://arxiv.org/abs/2312.02145", "accessDate": "2025-10-26T05:36:32Z", "archive": "", "archiveLocation": "", "shortTitle": "", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2312.02145 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-10-26T05:36:32Z", "dateModified": "2025-10-26T05:36:32Z"}}, {"key": "8QG4WK4Y", "version": 486, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/8QG4WK4Y", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/8QG4WK4Y", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/W7RHQTEU", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 6070551}}, "meta": {"creatorSummary": "Lei et al.", "parsedDate": "2025-10-13", "numChildren": 3}, "data": {"key": "8QG4WK4Y", "version": 486, "itemType": "preprint", "title": "MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps", "creators": [{"creatorType": "author", "firstName": "Jiahui", "lastName": "Lei"}, {"creatorType": "author", "firstName": "Kyle", "lastName": "Genova"}, {"creatorType": "author", "firstName": "George", "lastName": "Kopanas"}, {"creatorType": "author", "firstName": "Noah", "lastName": "Snavely"}, {"creatorType": "author", "firstName": "Leonidas", "lastName": "Guibas"}], "abstractNote": "This paper addresses the challenge of learning semantically and functionally meaningful 3D motion priors from real-world videos, in order to enable prediction of future 3D scene motion from a single input image. We propose a novel pixel-aligned Motion Map (MoMap) representation for 3D scene motion, which can be generated from existing generative image models to facilitate efficient and effective motion prediction. To learn meaningful distributions over motion, we create a large-scale database of MoMaps from over 50,000 real videos and train a diffusion model on these representations. Our motion generation not only synthesizes trajectories in 3D but also suggests a new pipeline for 2D video synthesis: first generate a MoMap, then warp an image accordingly and complete the warped point-based renderings. Experimental results demonstrate that our approach generates plausible and semantically consistent 3D scene motion.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2510.11107", "place": "", "date": "2025-10-13", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2510.11107", "citationKey": "", "url": "http://arxiv.org/abs/2510.11107", "accessDate": "2025-10-26T05:17:46Z", "archive": "", "archiveLocation": "", "shortTitle": "MoMaps", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2510.11107 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": ["8U6JPDHT"], "relations": {}, "dateAdded": "2025-10-26T05:17:46Z", "dateModified": "2025-10-26T05:17:46Z"}}, {"key": "4BEKMQ6U", "version": 37, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/4BEKMQ6U", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/4BEKMQ6U", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/8L44IFAF", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 3931092}}, "meta": {"creatorSummary": "Ren et al.", "parsedDate": "2024-01-25", "numChildren": 2}, "data": {"key": "4BEKMQ6U", "version": 37, "itemType": "preprint", "title": "Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks", "creators": [{"creatorType": "author", "firstName": "Tianhe", "lastName": "Ren"}, {"creatorType": "author", "firstName": "Shilong", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Ailing", "lastName": "Zeng"}, {"creatorType": "author", "firstName": "Jing", "lastName": "Lin"}, {"creatorType": "author", "firstName": "Kunchang", "lastName": "Li"}, {"creatorType": "author", "firstName": "He", "lastName": "Cao"}, {"creatorType": "author", "firstName": "Jiayu", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Xinyu", "lastName": "Huang"}, {"creatorType": "author", "firstName": "Yukang", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Feng", "lastName": "Yan"}, {"creatorType": "author", "firstName": "Zhaoyang", "lastName": "Zeng"}, {"creatorType": "author", "firstName": "Hao", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Feng", "lastName": "Li"}, {"creatorType": "author", "firstName": "Jie", "lastName": "Yang"}, {"creatorType": "author", "firstName": "Hongyang", "lastName": "Li"}, {"creatorType": "author", "firstName": "Qing", "lastName": "Jiang"}, {"creatorType": "author", "firstName": "Lei", "lastName": "Zhang"}], "abstractNote": "We introduce Grounded SAM, which uses Grounding DINO as an open-set object detector to combine with the segment anything model (SAM). This integration enables the detection and segmentation of any regions based on arbitrary text inputs and opens a door to connecting various vision models. As shown in Fig.1, a wide range of vision tasks can be achieved by using the versatile Grounded SAM pipeline. For example, an automatic annotation pipeline based solely on input images can be realized by incorporating models such as BLIP and Recognize Anything. Additionally, incorporating Stable-Diffusion allows for controllable image editing, while the integration of OSX facilitates promptable 3D human motion analysis. Grounded SAM also shows superior performance on open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in the wild) zero-shot benchmark with the combination of Grounding DINO-Base and SAM-Huge models.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2401.14159", "place": "", "date": "2024-01-25", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2401.14159", "citationKey": "", "url": "http://arxiv.org/abs/2401.14159", "accessDate": "2025-10-26T04:50:06Z", "archive": "", "archiveLocation": "", "shortTitle": "Grounded SAM", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2401.14159 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-10-26T04:50:06Z", "dateModified": "2025-10-26T04:50:06Z"}}, {"key": "43U9RZLN", "version": 496, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/43U9RZLN", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/43U9RZLN", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/CBKZULSI", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 3953853}}, "meta": {"creatorSummary": "Li et al.", "parsedDate": "2025-02-28", "numChildren": 3}, "data": {"key": "43U9RZLN", "version": 496, "itemType": "preprint", "title": "Enhancing End-to-End Autonomous Driving with Latent World Model", "creators": [{"creatorType": "author", "firstName": "Yingyan", "lastName": "Li"}, {"creatorType": "author", "firstName": "Lue", "lastName": "Fan"}, {"creatorType": "author", "firstName": "Jiawei", "lastName": "He"}, {"creatorType": "author", "firstName": "Yuqi", "lastName": "Wang"}, {"creatorType": "author", "firstName": "Yuntao", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Zhaoxiang", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Tieniu", "lastName": "Tan"}], "abstractNote": "In autonomous driving, end-to-end planners directly utilize raw sensor data, enabling them to extract richer scene features and reduce information loss compared to traditional planners. This raises a crucial research question: how can we develop better scene feature representations to fully leverage sensor data in end-to-end driving? Self-supervised learning methods show great success in learning rich feature representations in NLP and computer vision. Inspired by this, we propose a novel self-supervised learning approach using the LAtent World model (LAW) for end-to-end driving. LAW predicts future scene features based on current features and ego trajectories. This self-supervised task can be seamlessly integrated into perception-free and perception-based frameworks, improving scene feature learning and optimizing trajectory prediction. LAW achieves state-of-the-art performance across multiple benchmarks, including real-world open-loop benchmark nuScenes, NAVSIM, and simulator-based closed-loop benchmark CARLA. The code is released at https://github.com/BraveGroup/LAW.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2406.08481", "place": "", "date": "2025-02-28", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2406.08481", "citationKey": "", "url": "http://arxiv.org/abs/2406.08481", "accessDate": "2025-10-26T04:47:36Z", "archive": "", "archiveLocation": "", "shortTitle": "", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2406.08481 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": ["4KAX5SGB"], "relations": {}, "dateAdded": "2025-10-26T04:47:36Z", "dateModified": "2025-10-26T04:47:36Z"}}, {"key": "44F4VRMC", "version": 20, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/44F4VRMC", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/44F4VRMC", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/95SSRRWN", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 8274228}}, "meta": {"creatorSummary": "Mildenhall et al.", "parsedDate": "2020-08-03", "numChildren": 3}, "data": {"key": "44F4VRMC", "version": 20, "itemType": "preprint", "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "creators": [{"creatorType": "author", "firstName": "Ben", "lastName": "Mildenhall"}, {"creatorType": "author", "firstName": "Pratul P.", "lastName": "Srinivasan"}, {"creatorType": "author", "firstName": "Matthew", "lastName": "Tancik"}, {"creatorType": "author", "firstName": "Jonathan T.", "lastName": "Barron"}, {"creatorType": "author", "firstName": "Ravi", "lastName": "Ramamoorthi"}, {"creatorType": "author", "firstName": "Ren", "lastName": "Ng"}], "abstractNote": "We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2003.08934", "place": "", "date": "2020-08-03", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2003.08934", "citationKey": "", "url": "http://arxiv.org/abs/2003.08934", "accessDate": "2025-10-26T04:34:13Z", "archive": "", "archiveLocation": "", "shortTitle": "NeRF", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2003.08934 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}, {"tag": "Computer Science - Graphics", "type": 1}], "collections": [], "relations": {}, "dateAdded": "2025-10-26T04:34:13Z", "dateModified": "2025-10-26T04:34:13Z"}}, {"key": "X7TMFG3Z", "version": 489, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/X7TMFG3Z", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/X7TMFG3Z", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/CL6SV65D", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 1829991}}, "meta": {"creatorSummary": "Zheng et al.", "parsedDate": "2025-07-01", "numChildren": 3}, "data": {"key": "X7TMFG3Z", "version": 489, "itemType": "preprint", "title": "World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model", "creators": [{"creatorType": "author", "firstName": "Yupeng", "lastName": "Zheng"}, {"creatorType": "author", "firstName": "Pengxuan", "lastName": "Yang"}, {"creatorType": "author", "firstName": "Zebin", "lastName": "Xing"}, {"creatorType": "author", "firstName": "Qichao", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Yuhang", "lastName": "Zheng"}, {"creatorType": "author", "firstName": "Yinfeng", "lastName": "Gao"}, {"creatorType": "author", "firstName": "Pengfei", "lastName": "Li"}, {"creatorType": "author", "firstName": "Teng", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Zhongpu", "lastName": "Xia"}, {"creatorType": "author", "firstName": "Peng", "lastName": "Jia"}, {"creatorType": "author", "firstName": "Dongbin", "lastName": "Zhao"}], "abstractNote": "End-to-end autonomous driving directly generates planning trajectories from raw sensor data, yet it typically relies on costly perception supervision to extract scene information. A critical research challenge arises: constructing an informative driving world model to enable perception annotation-free, end-to-end planning via self-supervised learning. In this paper, we present World4Drive, an end-to-end autonomous driving framework that employs vision foundation models to build latent world models for generating and evaluating multi-modal planning trajectories. Specifically, World4Drive first extracts scene features, including driving intention and world latent representations enriched with spatial-semantic priors provided by vision foundation models. It then generates multi-modal planning trajectories based on current scene features and driving intentions and predicts multiple intention-driven future states within the latent space. Finally, it introduces a world model selector module to evaluate and select the best trajectory. We achieve perception annotation-free, end-to-end planning through self-supervised alignment between actual future observations and predicted observations reconstructed from the latent space. World4Drive achieves state-of-the-art performance without manual perception annotations on both the open-loop nuScenes and closed-loop NavSim benchmarks, demonstrating an 18.1\\% relative reduction in L2 error, 46.7% lower collision rate, and 3.75 faster training convergence. Codes will be accessed at https://github.com/ucaszyp/World4Drive.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2507.00603", "place": "", "date": "2025-07-01", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2507.00603", "citationKey": "", "url": "http://arxiv.org/abs/2507.00603", "accessDate": "2025-10-26T04:25:51Z", "archive": "", "archiveLocation": "", "shortTitle": "World4Drive", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2507.00603 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}], "collections": ["4KAX5SGB"], "relations": {}, "dateAdded": "2025-10-26T04:25:51Z", "dateModified": "2025-10-26T04:25:51Z"}}, {"key": "SJ59HNTN", "version": 482, "library": {"type": "user", "id": 18611688, "name": "Eric_Zhang_007", "links": {"alternate": {"href": "https://www.zotero.org/eric_zhang_007", "type": "text/html"}}}, "links": {"self": {"href": "https://api.zotero.org/users/18611688/items/SJ59HNTN", "type": "application/json"}, "alternate": {"href": "https://www.zotero.org/eric_zhang_007/items/SJ59HNTN", "type": "text/html"}, "attachment": {"href": "https://api.zotero.org/users/18611688/items/YUKPJ7CZ", "type": "application/json", "attachmentType": "application/pdf", "attachmentSize": 3631437}}, "meta": {"creatorSummary": "Chen et al.", "parsedDate": "2024-02-20", "numChildren": 3}, "data": {"key": "SJ59HNTN", "version": 482, "itemType": "preprint", "title": "VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning", "creators": [{"creatorType": "author", "firstName": "Shaoyu", "lastName": "Chen"}, {"creatorType": "author", "firstName": "Bo", "lastName": "Jiang"}, {"creatorType": "author", "firstName": "Hao", "lastName": "Gao"}, {"creatorType": "author", "firstName": "Bencheng", "lastName": "Liao"}, {"creatorType": "author", "firstName": "Qing", "lastName": "Xu"}, {"creatorType": "author", "firstName": "Qian", "lastName": "Zhang"}, {"creatorType": "author", "firstName": "Chang", "lastName": "Huang"}, {"creatorType": "author", "firstName": "Wenyu", "lastName": "Liu"}, {"creatorType": "author", "firstName": "Xinggang", "lastName": "Wang"}], "abstractNote": "Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.", "genre": "", "repository": "arXiv", "archiveID": "arXiv:2402.13243", "place": "", "date": "2024-02-20", "series": "", "seriesNumber": "", "DOI": "10.48550/arXiv.2402.13243", "citationKey": "", "url": "http://arxiv.org/abs/2402.13243", "accessDate": "2025-10-26T04:24:27Z", "archive": "", "archiveLocation": "", "shortTitle": "VADv2", "language": "", "libraryCatalog": "arXiv.org", "callNumber": "", "rights": "", "extra": "arXiv:2402.13243 [cs]", "tags": [{"tag": "Computer Science - Computer Vision and Pattern Recognition", "type": 1}, {"tag": "Computer Science - Robotics", "type": 1}], "collections": ["4KAX5SGB"], "relations": {}, "dateAdded": "2025-10-26T04:24:28Z", "dateModified": "2025-10-26T04:24:28Z"}}]